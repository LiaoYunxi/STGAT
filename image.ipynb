{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87df52b7-5b01-4cd2-b1db-e97c93f15890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "sys.path.append(\"/data/lyx/software/StereoMMv1/\")\n",
    "from process_img import *\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # 假设data已经是处理好的张量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def save_img_feat(data,save_path,feat_file):\n",
    "    print('shape of image_feature：',data.shape)\n",
    "    data.to_pickle(os.path.join(save_path,feat_file))\n",
    "    \n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f58eb5d-41cc-4b49-bb75-f4b9428aae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 100\n",
    "crop_size = 150\n",
    "output='hest_data/image_out'\n",
    "name = \"TENX94\"\n",
    "name = \"\".join(name)\n",
    "label_txt_file=str(name+'.h5')\n",
    "adata_file=str(name+'.h5ad')\n",
    "tile_path = os.path.join(output,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85b7029e-ac0e-41d3-ac3a-ce94edce5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = \"/data/lyx/software/StereoMMv1/\"\n",
    "data_path = \"/data/lyx/hubs/Cpath/hest_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12069adb-0c9b-4b10-8106-9120765f0576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/lyx/hubs/Cpath/hest_data/TENX94.h5'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osp.join(data_path,label_txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de7e9727-40cc-4190-adcc-6c5602e2e8c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"barcode\": shape (6078, 1), type \"|O\"> barcode /barcode\n",
      "<HDF5 dataset \"coords\": shape (6078, 2), type \"<i8\"> coords /coords\n",
      "<HDF5 dataset \"img\": shape (6078, 224, 224, 3), type \"|u1\"> img /img\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(osp.join(data_path,label_txt_file), 'r') as f:\n",
    "# 遍历文件中的所有组和数据集\n",
    "    for key in f.keys():\n",
    "        print(f[key], key, f[key].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b9aef89-f3e2-41a8-8ddc-99fa87b8eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: (6078, 224, 224, 3)\n",
      "数据类型: uint8\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(osp.join(data_path,label_txt_file), \"r\") as f:\n",
    "    # 获取数据集对象\n",
    "    img_dataset = f[\"img\"]\n",
    "    \n",
    "    # 将数据集读取为NumPy数组\n",
    "    img_data = img_dataset[:]  # 或 np.array(img_dataset)\n",
    "    \n",
    "print(\"数据形状:\", img_data.shape)  # 应为 (6078, 224, 224, 3)\n",
    "print(\"数据类型:\", img_data.dtype)  # 应为 uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33b57cd9-f2f1-4313-b060-1305630433cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with h5py.File(osp.join(data_path,label_txt_file), \"r\") as f:\n",
    "#     img_dataset = f[\"img\"]\n",
    "#     chunk_size = 100  # 每批读取100张图像\n",
    "    \n",
    "#     for i in range(0, img_dataset.shape[0], chunk_size):\n",
    "#         batch = img_dataset[i:i+chunk_size, :, :, :]\n",
    "#         print(f\"已读取批次 {i//chunk_size}: 形状={batch.shape}\")\n",
    "#         # 在此处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0488e6de-7eb6-4a62-8c74-ba936fd0372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_com = models.resnet50(pretrained=False)\n",
    "pth_path = os.path.join(current_path, \"torch_pths/resnet50-19c8e357.pth\")\n",
    "model_com.load_state_dict(torch.load(pth_path))\n",
    "num_features = model_com.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fdbd9b7-eb49-4c67-a489-9e78206819dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### strip the last layer\n",
    "model = torch.nn.Sequential(*list(model_com.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41a0f639-4e34-4dcf-87cc-65d27b35e3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Linear(in_features=2048, out_features=1024, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62816fe2-541f-4c20-9fd5-84aeb7685c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# feature_extractor = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False)#embed_layer=ConvStem\n",
    "# feature_extractor.head = nn.Identity()\n",
    "# pth_path = os.path.join(current_path, \"torch_pths/CHIEF_CTransPath.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7709836f-71b6-456c-8554-55387b9c23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model(current_path,model_name = 'resnet50'):\n",
    "#     if model_name == 'resnet50':\n",
    "#         model_com = models.resnet50(pretrained=False)\n",
    "#         if 'StereoMMv1' in current_path:\n",
    "#             pth_path = os.path.join(current_path, \"torch_pths/resnet50-19c8e357.pth\")\n",
    "#         else:\n",
    "#             pth_path = os.path.join(current_path, \"StereoMMv1/torch_pths/resnet50-19c8e357.pth\")\n",
    "#         model_com.load_state_dict(torch.load(pth_path))\n",
    "#         num_features = model_com.fc.in_features\n",
    "#         ### strip the last layer\n",
    "#         feature_extractor = torch.nn.Sequential(*list(model_com.children())[:-1])\n",
    "#     elif model_name == 'CHIEF':\n",
    "#         import timm\n",
    "#         feature_extractor = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False)#embed_layer=ConvStem\n",
    "#         feature_extractor.head = nn.Identity()\n",
    "#         if 'StereoMMv1' in current_path:\n",
    "#             pth_path = os.path.join(current_path, \"torch_pths/CHIEF_CTransPath.pth\")\n",
    "#         else:\n",
    "#             pth_path = os.path.join(current_path, \"StereoMMv1/torch_pths/CHIEF_CTransPath.pth\")\n",
    "#         td = torch.load(pth_path)\n",
    "#         feature_extractor.load_state_dict(td['model'], strict=True)\n",
    "\n",
    "#     feature_extractor.to(device)\n",
    "#     feature_extractor.eval()\n",
    "#     return(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f447f581-6b83-467e-bb59-3f90ec4bab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6078, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img_data = torch.from_numpy(img_data).permute(0, 3, 1, 2).float()  # 转换为浮点型\n",
    "print(img_data.shape)\n",
    "# 可选：归一化到 [0, 1] 范围（如果原始数据是 uint8 的 0-255）\n",
    "# tensor_data = tensor_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7630426-d4bb-4db3-b9f8-b415f2d662cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # 仅返回图像，无标签\n",
    "\n",
    "# 定义数据增强（根据任务选择）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96376bdb-5f23-40e4-8d23-1481dac6d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(\n",
    "    images=img_data,\n",
    "    transform=train_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cefb619-3019-4cd1-9424-e7c8126b67bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed09b0f6-c445-4804-84a6-f5ae601ee293",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f103b136-3cb8-4115-9857-4d2f37639a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_outputs = []\n",
    "model.eval()\n",
    "for batch in loader:\n",
    "    inputs = batch.to(device)\n",
    "    outputs = model(inputs)\n",
    "    outputs = outputs.data.cpu().numpy().ravel()\n",
    "    feat_outputs.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f02289c-b4c3-4eb8-a5d3-05cc5c485f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = np.vstack(feat_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ea2a639c-7b57-4ba1-821f-e586c9a44dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved\n"
     ]
    }
   ],
   "source": [
    "# save_img_feat(img_emb,output,feat_file = 'img_feat.pkl')\n",
    "np.save(f\"{name}_resnet_img_emb.npy\",img_emb)\n",
    "print('Features saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9d389-d1af-409e-bf5a-ac58d79f8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器定义\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self,img_model):\n",
    "        super().__init__()\n",
    "        self.model = img_model\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1024)\n",
    "        self.fc = nn.Linear(in_features=2048, out_features=1024, bias=True)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat(self.pooling(x),self.fc(x),axis=1)\n",
    "        return F.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78438849-00d3-495d-a062-e3df0231b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptomeEncoder(nn.Module):\n",
    "    def __init__(self, input_dim,pretrainmodel):\n",
    "        super().__init__()\n",
    "        self.token_emb = pretrainmodel.token_emb\n",
    "        self.pos_emb = pretrainmodel.pos_emb\n",
    "        self.encoder = pretrainmodel.encoder\n",
    "        self.fc1 = nn.Linear(in_features=input_dim*2, out_features=1024, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=input_dim*2, out_features=1024, bias=True)  \n",
    "    def forward(self, x,position_gene_ids,x_padding):\n",
    "        x = self.token_emb(x, output_weight = 0)\n",
    "        position_emb = self.pos_emb(position_gene_ids)\n",
    "        x += position_emb\n",
    "        geneemb = self.encoder(x,x_padding)\n",
    "        geneemb1 = geneemb[:,-1,:]\n",
    "        geneemb2 = geneemb[:,-2,:]\n",
    "        geneemb3, _ = torch.max(geneemb[:,:-2,:], dim=1)\n",
    "        geneemb4 = torch.mean(geneemb[:,:-2,:], dim=1)\n",
    "        x1 = torch.concat([geneemb1,geneemb2],axis=1)\n",
    "        x2 = torch.concat([geneemb3,geneemb4],axis=1)\n",
    "        x = torch.concat([self.fc1(x1),self.fc1(x2)],axis=1)\n",
    "        return F.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e7c4e-0c20-4461-a279-a49926baf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgthighres = 'a5'\n",
    "# geneemb_list = []\n",
    "# for i in tqdm(range(gexpr_feature.shape[0])):\n",
    "#     with torch.no_grad():\n",
    "#         tmpdata = (np.log1p(gexpr_feature.iloc[i,:]/(gexpr_feature.iloc[i,:].sum())*1e4)).tolist()\n",
    "#         totalcount = gexpr_feature.iloc[i,:].sum()\n",
    "#         pretrain_gene_x = torch.tensor(tmpdata+[np.log10(totalcount)+float(tgthighres[1:]),np.log10(totalcount)]).unsqueeze(0)\n",
    "#         data_gene_ids = torch.arange(19266, device=pretrain_gene_x.device).repeat(pretrain_gene_x.shape[0], 1)\n",
    "        \n",
    "#         value_labels = pretrain_gene_x > 0\n",
    "#         x, x_padding = gatherData(pretrain_gene_x, value_labels, pretrainconfig['pad_token_id'])\n",
    "\n",
    "        # position_gene_ids, _ = gatherData(data_gene_ids, value_labels, pretrainconfig['pad_token_id'])\n",
    "        \n",
    "        x = pretrainmodel.token_emb(torch.unsqueeze(x, 2).float().to(device), output_weight = 0)\n",
    "        position_emb = pretrainmodel.pos_emb(position_gene_ids.to(device))\n",
    "        x += position_emb\n",
    "        geneemb = pretrainmodel.encoder(x,x_padding.to(device))\n",
    "        geneemb_list.append(geneemb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e9f3c-02c0-4be2-8539-b97b7010ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型与参数\n",
    "image_encoder = ImageEncoder()\n",
    "tx_encoder = TranscriptomeEncoder(input_dim=20000)\n",
    "logit_scale = nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': image_encoder.parameters()},\n",
    "    {'params': tx_encoder.parameters()},\n",
    "    {'params': [logit_scale], 'lr': 0.001}\n",
    "], lr=0.0001)\n",
    "\n",
    "# 训练循环\n",
    "for images, tx_data in dataloader:\n",
    "    img_emb = image_encoder(images)\n",
    "    tx_emb = tx_encoder(tx_data)\n",
    "    \n",
    "    # 计算相似度\n",
    "    logits = img_emb @ tx_emb.T * logit_scale.exp()\n",
    "    labels = torch.arange(len(images), device=images.device)\n",
    "    \n",
    "    # 对称损失\n",
    "    loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STED",
   "language": "python",
   "name": "sted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
